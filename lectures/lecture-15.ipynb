{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 15:  Matrix functions (cont.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "**Week 1:** Matrices, vectors, matrix/vector norms, scalar products & unitary matrices  \n",
    "**Week 2:** TAs-week (Strassen, FFT, a bit of SVD)  \n",
    "**Week 3:** Matrix ranks, singular value decomposition, linear systems, eigenvalues  \n",
    "**Week 4:** Matrix decompositions: QR, LU, SVD + test + structured matrices start  \n",
    "**Week 5:** Iterative methods, preconditioners, matrix functions\n",
    "**Week 6:** Advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "- Direct methods for Toeplitz matrices, Gohberg-Semencul formula\n",
    "- Iterative methods for Toeplitz matrices\n",
    "- Circulant preconditioners\n",
    "- Two-dimensional (BTTB) matrices\n",
    "- Matrix functions: basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "\n",
    "- Matrix functions and matrix equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix functions: definition\n",
    "\n",
    "One way to define a matrix function $f(A)$ is to use **Jordan canonical form**.\n",
    "\n",
    "A much more elegant way is to use **Cauchy integral representation:**\n",
    "\n",
    "$$\n",
    "    f(A) = \\int_{\\Gamma} f(z) (zI - A)^{-1} dz,\n",
    "$$\n",
    "where $f(z)$ is analytic on and inside a closed contour $\\Gamma$ that encloses the spectrum of $A$.\n",
    "\n",
    "This definition can be generalized to the **operator case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important matrix functions\n",
    "\n",
    "- Matrix exponential, used to solve $\\frac{dy}{dt} = Ay$ in the \"explicit\" way, $y = y_0 e^{At}.$\n",
    "- $\\cos(A), \\sin(A)$ used to solve wave equation $\\frac{d^2 y}{dt^2} + Ay = 0.$\n",
    "- Sign function, $\\mathrm{sign}(A)$, used to compute **spectral projections.**\n",
    "- Inverse square root $A^{-1/2}$ used in many places, for example, to generate samples from a Gaussian distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix exponential\n",
    "\n",
    "The matrix exponential is given by the following series:\n",
    "\n",
    "$$e^A = I + A + \\frac{1}{2} A^2 + \\frac{1}{3!} A^3 + \\ldots$$\n",
    "\n",
    "Is it a good idea to compute matrix exponential in this form (even in the scalar case?)\n",
    "\n",
    "This almost assumes a **Krylov method** for the evaluation of $e^{At} y_0,$ by the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 1.0 #Point\n",
    "k = 10 #Number of terms\n",
    "b = 1.0\n",
    "x0 = x\n",
    "for i in range(1, k):\n",
    "    b += x0\n",
    "    x0 *= x/(i+1)\n",
    "    \n",
    "print 'Error in the exponent:', b - np.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Series convergence\n",
    "The series convergence for the matrix exponential can be slow for large $x!$ (and slow for big condition number).\n",
    "\n",
    "What we can do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method 1: Krylov method\n",
    "\n",
    "We can use the idea of Krylov method: using the Arnoldi method, generate the orthogonal basis in the Krylov subspace,\n",
    "\n",
    "and compute (it can be used in general for any function)\n",
    "\n",
    "$$ f(A) \\approx f(Q H Q^*) = Q f(H) Q^*,$$\n",
    "\n",
    "where $H$ is a small upper Hessenberg matrix, for which we can use, for example, the **Schur-Parlett algorithm.**\n",
    "\n",
    "The convergence of the Krylov method can be quite slow: it is actually a **polynomial approximation** to a function.\n",
    "\n",
    "And convergence of polynomial approximation to the matrix exponent **can be slow.**\n",
    "\n",
    "**Idea:** Replace by rational approximation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pade approximations\n",
    "Matrix exponential is well approximated by **rational function**:  \n",
    "\n",
    "$$\\exp(x) \\approx \\frac{p(x)}{q(x)},$$\n",
    "\n",
    "where $p(x)$ and $q(x)$ are polynomials\n",
    "\n",
    "and computation of a rational function of a matrix is reduced to **matrix-matrix products** and m**matrix inversions**.  \n",
    "\n",
    "The rational form is also very useful when only a product of a matrix exponential by vector is needed, since  \n",
    "\n",
    "evaluation reduces to **matrix-by-vector products** and **linear systems solvers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Computing Pade approximant\n",
    "\n",
    "import sympy.mpmath\n",
    "%matplotlib inline\n",
    "from sympy.mpmath import pade, taylor, polyval\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(0, 1, 128)\n",
    "a = taylor(sympy.mpmath.exp, 0, 100) #Taylor series\n",
    "p, q = pade(a, 6, 6) #Pade approximant\n",
    "plt.plot(x, polyval(p[::-1], x)/polyval(q[::-1], x) - np.exp(x))\n",
    "plt.title('Error of the Pade approximation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scaling & squaring algorithm\n",
    "\n",
    "The \"canonical algorithm\" also relies on **scaling** of the matrix $A:$\n",
    "\n",
    "$$\\exp(A) = \\exp(A/2^k)^(2^k),$$\n",
    "\n",
    "so you:\n",
    "\n",
    "- Scale the matrix as $B := A/2^k$ to make it norm less than $1$.\n",
    "- Compute exponent of $C = e^B$ by a **Pade approximant**\n",
    "- Square $e^A \\approx C^(2^k)$ in $k$ matrix-by-matrix products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Large-scale matrix exponentials\n",
    "\n",
    "Large-scale matrices obviously do not allow for efficient scaling-and-squaring (need to work with dense matrices), \n",
    "\n",
    "thus we can use **Krylov methods** or (better) Rational Krylov methods.\n",
    "\n",
    "The simplest (yet efficient) approach is based on the so-called **extended Krylov subspaces:**\n",
    "\n",
    "$$KE(A, b) = \\mathrm{Span}(\\ldots, A^{-2} b, A^{-1} b, b, A b, A^2 b, \\ldots)$$\n",
    "\n",
    "At each step you add a vector of the form $A w$ and $A^{-1} w$ to the subspace, and orthogonalize the result (**rational Arnoldi method).\n",
    "\n",
    "I.e., we need only linear system solver for one step, but since the matrix $A$ is fixed, we can **factorize it** once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rational Krylov methods\n",
    "\n",
    "Rational Krylov methods are the most efficient for the computation of matrix functions:\n",
    "\n",
    "we construct an orthogonal basis in the span,\n",
    "\n",
    "$$KE(A, b) = \\mathrm{Span}(\\ldots, A^{-2} b, A^{-1} b, b, A b, A^2 b, \\ldots)$$\n",
    "\n",
    "and compute\n",
    "\n",
    "$$f(A) \\approx Q f(H) Q^*,$$\n",
    "\n",
    "where $H = Q^* A Q.$\n",
    "\n",
    "It requires one solver and matrix-by-vector product at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application to graph centrality\n",
    "\n",
    "Important application of matrix exponent is solution of PDes; However, there are other applications, for example, in graph theory, in computation of the **graph centrality**.\n",
    "\n",
    "This example was taken from [here](http://nbviewer.ipython.org/github/sdrelton/matrix_function_notebooks/blob/master/TheMatrixExponential.ipynb).\n",
    "Take the following network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "Adj = np.array([[0, 1, 1, 0, 0, 0], \n",
    "                [1, 0, 1, 1, 1, 0], \n",
    "                [1, 1, 0, 0, 0, 0], \n",
    "                [0, 1, 0, 0, 0, 1], \n",
    "                [0, 1, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 1, 0, 0]])\n",
    "\n",
    "G = nx.from_numpy_matrix(Adj)\n",
    "nx.draw(G, node_color='y', node_size=1000, with_labels=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One measure of the importance of each node is its <b>centrality</b>.\n",
    "\n",
    "We can count the number of paths of different lengths from $i$ to $i$,  \n",
    "and add them up to get **centrality** of the node\n",
    "\n",
    "$$c(i) = \\alpha_1 A_{ii} + \\alpha_2 A^2_{ii} + \\alpha_3 A^3_{ii} + \\cdots,$$\n",
    "\n",
    "where the coefficients $\\alpha_k$ remain to be chosen.  \n",
    "\n",
    "With $\\alpha_k = \\frac{1}{k!}$ we get  \n",
    "\n",
    "$$\n",
    "    c = \\mathrm{diag}(\\exp(A))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "centralities = np.diag(sla.expm(np.array(Adj, dtype=np.double)))\n",
    "nodeorder = np.argsort(centralities)[::-1]\n",
    "\n",
    "print np.array([nodeorder, centralities[nodeorder]])\n",
    "\n",
    "# Note: This is already built into networkx using the following command\n",
    "# print nx.communicability_centrality_exp(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other matrix functions\n",
    "\n",
    "Now, let us briefly talk about **other** matrix functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sign function\n",
    "\n",
    "Sign function is defined as\n",
    "\n",
    "$$\\mathrm{sign}(x) = \\begin{cases} 1, \\quad x > 0, \\\\ -1, \\quad x < 0. \\end{cases}$$\n",
    "\n",
    "Given a matrix $A = U \\Lambda U^*$, it effectively puts all the eigenvalues larger than $0$ to $1$, and all eigenvalues smaller than $0$ to $-1$, thus\n",
    "\n",
    "$$P = \\frac{(I + \\mathrm{sign}(A))}{2}$$\n",
    "\n",
    "is a **projector** onto the subspace spanned by all positive eigenvalues. \n",
    "\n",
    "Such projectors can be very useful in **large-scale** eigenvalue computations, when you only need to find a subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to compute sign function?\n",
    "\n",
    "There is a very simple iteration to compute the sign function, namely\n",
    "\n",
    "$$X_{k+1} = \\frac{1}{2} (X_k + X^{-1}_k), X_0 = \\alpha A.$$\n",
    "\n",
    "This iteration converges quadratically to the sign function.\n",
    "\n",
    "You can also get a polynomial iteration, [proposed by R. Byers](http://www.sciencedirect.com/science/article/pii/0024379587902229)\n",
    "\n",
    "$$X_{k+1} = \\frac{1}{2} X_k (3 I - X_k), \\quad X_0 = \\alpha A.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix sign function: applications\n",
    "\n",
    "One of the important applications of the matrix sign function is the solution of the **Algebraic Riccati equation**\n",
    "\n",
    "$$A^* X + X A - X R X + G = 0,$$\n",
    "\n",
    "which arises in optimal control and stochastic control.\n",
    "\n",
    "Solving **ARE** is equivalent to finding a **stable** invariant subspace (i.e., corresponding to the negative eigenvalues)\n",
    "\n",
    "of the matrix\n",
    "\n",
    "$$\n",
    "    C = \\begin{bmatrix} A^* & G \\\\ R & -A \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse square root of the matrix\n",
    "\n",
    "The inverse square root of the matrix, $A^{-1/2}$ is also often important.\n",
    "\n",
    "For example, the multidimensional Gaussian distribution with covariance matrix $A = A^* > 0$ is given by the \n",
    "\n",
    "$$e^{A^{-1} x, x},$$\n",
    "\n",
    "Suppose $x$ is really huge (millions), how we **generate samples**, given a structured matrix $A$?\n",
    "\n",
    "The simplest algorithm is to generate a normally distributed vector $y$ with $y_i$ from $N(0, 1)$, and then compute\n",
    "\n",
    "$$x = A^{-\\frac{1}{2}} y.$$\n",
    "\n",
    "The vector $x$ will have the desired distribution.\n",
    "\n",
    "To compute matrix square root it is very efficient to use **rational Krylov subspaces.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "- (First part): The Schur-Parlett algorithm for matrix functions\n",
    "- (Second part): More details on the matrix exponential, rational Krylov subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced topics\n",
    "\n",
    "In the previous lectures, we have covered the basics of the NLA:\n",
    "\n",
    "- Concept of floating point and blocking\n",
    "- Matrix decompositions and how to solve linear systems with them, do eigenvalue computations\n",
    "- Sparse / structured matrix techiques, matrix functions\n",
    "\n",
    "Now we will try to discuss more recent \"research topics\" in linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Some) of the ongoing research in numerical linear algebra\n",
    "\n",
    "\n",
    "- Many of the classical problems are solved, but improvements are always possible\n",
    "- Generalization from the matrix case to the multidimensional case (multilinear algebra, lecture on Thursday this week)\n",
    "- Using non-standard approximation norms (compressed sensing, L1-norm minimization, why L1-norm minimization gives the sparsest solutions)\n",
    "- Using block-low-rank approximation to construct efficient solvers for large-scale sparse matrices\n",
    "- Many others (like solvers for the finite fields).\n",
    "\n",
    "Today we will talk about large-scale low-rank approximation and maximum-volume principle, and also show \n",
    "the connection to the **optimal design problems** that can be traced back to Dijkstra, Fedorov and many others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Low-rank approximation\n",
    "\n",
    "Let us go back to the beginning of the course, and consider a large $n \\times n$ matrix $A$ of rank-$r \\ll n$.\n",
    "\n",
    "Then, if there is a **non-singular submatrix** of the matrix $A$, we can decompose this matrix as\n",
    "\n",
    "$$A = C \\widehat{A}^{-1} R,$$\n",
    "\n",
    "where $C$ are some **columns** of $A$, $R$ are some **rows** of $A$ and $\\widehat{A}$ is a (non-singular) submatrix on their intersection.\n",
    "\n",
    "\n",
    "Now suppose the matrix $A$ is only **approximated** with a rank-$r$ matrix, and $\\widehat{A}^{-1}$ is **close to singular**.\n",
    "\n",
    "Then, if we select a wrong submatrix, the error might be huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 50\n",
    "r = 10\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)]\n",
    "a = np.array(a)\n",
    "\n",
    "ind = np.arange(r, dtype=np.int)\n",
    "sbm = a[ind, :][:, ind]\n",
    "col = a[:, ind]\n",
    "row = a[ind, :]\n",
    "\n",
    "approx = col.dot(np.linalg.inv(sbm)).dot(row)\n",
    "print 'Approximation error:', np.linalg.norm(a - approx)/np.linalg.norm(a)\n",
    "u, s, v = np.linalg.svd(a)\n",
    "u = u[:, :r]\n",
    "s = s[:r]\n",
    "v = v[:r, :]\n",
    "approx_svd = u.dot(np.diag(s)).dot(v)\n",
    "print 'SVD approximation error:', np.linalg.norm(a - approx_svd)/np.linalg.norm(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can actually do much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tt.maxvol import maxvol #Our own ttpy package \n",
    "n = 50\n",
    "r = 10\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)]\n",
    "a = np.array(a)\n",
    "u, s, v = np.linalg.svd(a)\n",
    "u = u[:, :r]\n",
    "s = s[:r]\n",
    "v = v[:r, :]\n",
    "approx_svd = u.dot(np.diag(s)).dot(v)\n",
    "\n",
    "indu = maxvol(u) #Optimal indices\n",
    "approx_maxvol = a[:, indu].dot(np.linalg.solve(a[:, indu][indu, :], a[indu, ])) #It is more stable formula\n",
    "print 'Maxvol approximation error:', np.linalg.norm(a - approx_maxvol)/np.linalg.norm(a)\n",
    "print 'SVD approximation error:', np.linalg.norm(a - approx_svd)/np.linalg.norm(a)\n",
    "\n",
    "plt.plot(np.sort(indu), np.ones(r), 'x')\n",
    "plt.title('Distribution of interpolation nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is behind these experiments\n",
    "\n",
    "Behind this is the **maximum volume principle** in the approximation by low-rank matrices.\n",
    "\n",
    "The submatrix $\\widehat{A}$ has to be selected in such a way that it is volume, defined as \n",
    "\n",
    "$$V(\\widehat{A}) = | \\det (A) | .$$\n",
    "\n",
    "\n",
    "is larger.\n",
    "\n",
    "The exact theorem, proved by Goreinov and Tyrtyshnikov, has the form\n",
    "\n",
    "$$|A - A_{skel}|_{ij} \\leq (r + 1) \\sigma_{r+1}(A).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why maximum volume principle?\n",
    "\n",
    "Let us go back to the **index form** of the low-rank approximation:\n",
    "\n",
    "$$A_{ij} = \\sum_{\\alpha=1}^r U_{i \\alpha} V_{j \\alpha}.$$\n",
    "\n",
    "Suppose, we fix $V$.\n",
    "\n",
    "Then we have many equations for few unknowns, let us select just $r$ of those to get a **non-singular system.**\n",
    "\n",
    "Then, the skeleton decomposition is just the selection of a certain subset of indices $j_k$ of size $r$, \n",
    "\n",
    "such that the matrix \n",
    "\n",
    "$$ \\widehat{V}_{k \\alpha} = V_{j_k, \\alpha}$$ is non-singular!\n",
    "\n",
    "This generalizes to the problem of **optimal interpolation**, if we replace a disrete variable by a continious one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimal interpolation\n",
    "\n",
    "Suppose you want to approximate a given function $f(x)$ by a linear combination of some **basis functions** $\\phi_j(x)$ that we know,\n",
    "\n",
    "$$f(x) \\approx \\sum_{j=1}^r c_j \\phi_j(x).$$\n",
    "\n",
    "Variable $x$ can be anything: one-dimensional, two-dimensional, \\ldots.\n",
    "\n",
    "This can be formulated as **linear least squares problem.** \n",
    "\n",
    "A much simpler approach is to design a set of **interpolation points** $x_s, \\quad s = 1, \\ldots, r$ such that\n",
    "\n",
    "the coefficients can be recovered from **intepolation:**\n",
    "\n",
    "$$f(x_s) = \\sum_{j=1}^r c_j \\phi_j(x_s).$$\n",
    "\n",
    "What are the best interpolation points, given the functions $\\phi_j(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Leya points\n",
    "\n",
    "The Leya points are defined as the one that maximize the volume,\n",
    "\n",
    "$$| \\det [\\phi_j(x_s)] |.$$\n",
    "\n",
    "In this case, we get a provable error bound on the approximation:\n",
    "\n",
    "$$E_{interp} \\leq (r + 1) E_{best}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to matrices\n",
    "\n",
    "If we now replace $x$ by a large set of **discrete indices**, we are left with a large overdetermined system of linear equations\n",
    "\n",
    "\n",
    "$$f = A c,$$\n",
    "\n",
    "and we want to find the submatrix in $A$ that corresponds to the **maximal volume.**\n",
    "\n",
    "How we can do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maxvol-algorithm\n",
    "\n",
    "\n",
    "Given an $n \\times r$ matrix, find the submatrix of largest **volume** it in.\n",
    "\n",
    "Use greedy algorithm(!)\n",
    "- Take some rows, put them in the first $r$. Compute $B = A \\widehat{A}^{-1}$\n",
    "- $B = \\begin{pmatrix} I \\\\\n",
    "    Z \\end{pmatrix}$\n",
    "- Suppose maximal element in $Z$ is in position $(i,j)$. \n",
    "- Swap $i$-th row with $j$-th row.\n",
    "- Stop if maximal element is less than $(1+\\delta)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notes on the convergence of the greedy method\n",
    "\n",
    "We can prove some estimates on the convergence, and some estimates on the **quasioptimality** of the result.\n",
    "\n",
    "The quasioptimality is based on the fundamental ideas of **supermodular functions.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maxvol and optimal design\n",
    "\n",
    "In fact, we want to do some measurements on a system at some points, probably to minimize the error we get.\n",
    "\n",
    "This is a standard problem in **optimal design.**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimal design \n",
    "\n",
    "Suppose we have a \"linear experiment\" with the design matrix $X:$\n",
    "\n",
    "$$ y = X \\beta + \\varepsilon.$$\n",
    "\n",
    "The rows of the **design matrix** correpond to the particular experiment. (i.e., all possible experiments).\n",
    "\n",
    "We want to select the \"best\" experiments, so we need to make as few measurements, as possible, to fit $\\beta$.\n",
    "\n",
    "There are different optimization criteria, and $D$-optimality (maxvol) is one of them.\n",
    "\n",
    "We can also generalize to arbitrary regression with the help of **Fisher information matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Fisher information matrix\n",
    "\n",
    "In the general case, we are given a random variable $X$ and a set of (unknown) parameters $\\theta = [\\theta_1, \\ldots, \\theta_N]$.\n",
    "\n",
    "We want to measure, what is the information in the measurement of the random variable $X$, were $f$ is the probability function for $X$. The logarithm of $f$ is called **score**, and its second-order moment is the **Fisher information matrix:**\n",
    "\n",
    "$$I(\\theta)_{ij} = E(\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log f(X, \\theta) \\mid \\theta)$$\n",
    "\n",
    "For a normal distribution, Fisher information boils down to the inverse of the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rectangular case\n",
    "\n",
    "For the rectangular case (we select more rows than columns), one has to maximuze \n",
    "\n",
    "$$\\det A^* A.$$\n",
    "\n",
    "It can be applied to such tasks as recommender systems, feature selection, and even iterative methods for linear least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This lecture had two parts:\n",
    "\n",
    "- Matrix functions part\n",
    "- Maxvol part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "\n",
    "- Compressed sensing, L1-norm minimization, matching pursuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
