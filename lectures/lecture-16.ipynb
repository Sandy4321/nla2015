{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 16:  Intro to compressed sensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "**Week 1:** Matrices, vectors, matrix/vector norms, scalar products & unitary matrices  \n",
    "**Week 2:** TAs-week (Strassen, FFT, a bit of SVD)  \n",
    "**Week 3:** Matrix ranks, singular value decomposition, linear systems, eigenvalues  \n",
    "**Week 4:** Matrix decompositions: QR, LU, SVD + test + structured matrices start  \n",
    "**Week 5:** Iterative methods, preconditioners, matrix functions  \n",
    "**Week 6:** Advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "- Matrix functions (matrix sign function, matrix square root, some application of the matrix exponential, Rational Krylov methods)\n",
    "- Maxvol and optimal design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "\n",
    "- Compressed sensing, L1-norm minimization, matching pursuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic problem setting \n",
    "\n",
    "In many applications, we have to deal with **insufficient information,**  i.e. the number of equations is less than the number of parameters.  \n",
    "\n",
    "For example:\n",
    "\n",
    "- **Denoising problem:** you are given a \"noisy\" image $y = x + \\eta,$ and you do not know both $x$ and $\\eta$, you only know that $\\eta$ is \"noise\".\n",
    "- **Deblurring / deconvolution:** $y  = Ax + \\eta$, the matrix $A$ is ill-conditioned (i.e. close to low-rank matrix), thus there are few effective equations\n",
    "- **Superresolution:** you are given a low-resolution representation of an image, $y = B x$, where $B$ is rectangular \"sampling\" operator.\n",
    "\n",
    "- Inverse problem in general (recover the material coefficients from the measurements)\n",
    "\n",
    "You have to assume some **prior** information on your solution $x$, since there are many **admissable** solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard case: Tikhonov regularization\n",
    "\n",
    "The most widely used **regularization** is the so-called Tikhonov regularization (or minimum-norm solution).\n",
    "\n",
    "Given an underdetermined linear system \n",
    "\n",
    "$$Ax = y,$$ where  $A$ is $n \\times m$ and $n < m$, \n",
    "\n",
    "there are many solutions; the **minimial norm** solution is defined as the one having minimal $2$-norm:\n",
    "\n",
    "$$x^* = \\arg \\min_{\\mbox{s.t.} Ax = y} \\Vert x \\Vert_2,$$\n",
    "\n",
    "and explicit solution is given by the **pseudoinverse:**\n",
    "\n",
    "$$x^* = A^{\\dagger} y.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other norms\n",
    "\n",
    "However, the solution will be \"smoothed\" with this, destroying the original features. \n",
    "\n",
    "Many of the signals are \"sparse\", in the sense they have many zeros. \n",
    "\n",
    "This motivates the definition of the \"zero-norm\" (which is not actually a norm):\n",
    "\n",
    "$$\\Vert x \\Vert_0 = \\# \\{k: x_k \\ne 0 \\}.$$\n",
    "\n",
    "(the number of non-zero elements). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## L1-norm\n",
    "\n",
    "Minimization of zero-norm is a combinatorial task.\n",
    "\n",
    "A **convex relaxation** is the 1-norm. That gives the following optimization problem:\n",
    "\n",
    "$$x^* = \\arg \\min_{\\mbox{s.t.} Ax = y} \\Vert x \\Vert_1,$$\n",
    "\n",
    "Which is a **linear programming task.**\n",
    "\n",
    "Let us see, what will be the solutions for a synthetic problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     49,
     56,
     65,
     98
    ],
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load plot_tomography_l1_reconstruction.py\n",
    "\"\"\"\n",
    "======================================================================\n",
    "Compressive sensing: tomography reconstruction with L1 prior (Lasso)\n",
    "======================================================================\n",
    "\n",
    "This example shows the reconstruction of an image from a set of parallel\n",
    "projections, acquired along different angles. Such a dataset is acquired in\n",
    "**computed tomography** (CT).\n",
    "\n",
    "Without any prior information on the sample, the number of projections\n",
    "required to reconstruct the image is of the order of the linear size\n",
    "``l`` of the image (in pixels). For simplicity we consider here a sparse\n",
    "image, where only pixels on the boundary of objects have a non-zero\n",
    "value. Such data could correspond for example to a cellular material.\n",
    "Note however that most images are sparse in a different basis, such as\n",
    "the Haar wavelets. Only ``l/7`` projections are acquired, therefore it is\n",
    "necessary to use prior information available on the sample (its\n",
    "sparsity): this is an example of **compressive sensing**.\n",
    "\n",
    "The tomography projection operation is a linear transformation. In\n",
    "addition to the data-fidelity term corresponding to a linear regression,\n",
    "we penalize the L1 norm of the image to account for its sparsity. The\n",
    "resulting optimization problem is called the :ref:`lasso`. We use the\n",
    "class :class:`sklearn.linear_model.Lasso`, that uses the coordinate descent\n",
    "algorithm. Importantly, this implementation is more computationally efficient\n",
    "on a sparse matrix, than the projection operator used here.\n",
    "\n",
    "The reconstruction with L1 penalization gives a result with zero error\n",
    "(all pixels are successfully labeled with 0 or 1), even if noise was\n",
    "added to the projections. In comparison, an L2 penalization\n",
    "(:class:`sklearn.linear_model.Ridge`) produces a large number of labeling\n",
    "errors for the pixels. Important artifacts are observed on the\n",
    "reconstructed image, contrary to the L1 penalization. Note in particular\n",
    "the circular artifact separating the pixels in the corners, that have\n",
    "contributed to fewer projections than the central disk.\n",
    "\"\"\"\n",
    "\n",
    "# Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>\n",
    "# License: BSD 3 clause\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _weights(x, dx=1, orig=0):\n",
    "    x = np.ravel(x)\n",
    "    floor_x = np.floor((x - orig) / dx)\n",
    "    alpha = (x - orig - floor_x * dx) / dx\n",
    "    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))\n",
    "\n",
    "\n",
    "def _generate_center_coordinates(l_x):\n",
    "    l_x = float(l_x)\n",
    "    X, Y = np.mgrid[:l_x, :l_x]\n",
    "    center = l_x / 2.\n",
    "    X += 0.5 - center\n",
    "    Y += 0.5 - center\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def build_projection_operator(l_x, n_dir):\n",
    "    \"\"\" Compute the tomography design matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    l_x : int\n",
    "        linear size of image array\n",
    "\n",
    "    n_dir : int\n",
    "        number of angles at which projections are acquired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : sparse matrix of shape (n_dir l_x, l_x**2)\n",
    "    \"\"\"\n",
    "    X, Y = _generate_center_coordinates(l_x)\n",
    "    angles = np.linspace(0, np.pi, n_dir, endpoint=False)\n",
    "    data_inds, weights, camera_inds = [], [], []\n",
    "    data_unravel_indices = np.arange(l_x ** 2)\n",
    "    data_unravel_indices = np.hstack((data_unravel_indices,\n",
    "                                      data_unravel_indices))\n",
    "    for i, angle in enumerate(angles):\n",
    "        Xrot = np.cos(angle) * X - np.sin(angle) * Y\n",
    "        inds, w = _weights(Xrot, dx=1, orig=X.min())\n",
    "        mask = np.logical_and(inds >= 0, inds < l_x)\n",
    "        weights += list(w[mask])\n",
    "        camera_inds += list(inds[mask] + i * l_x)\n",
    "        data_inds += list(data_unravel_indices[mask])\n",
    "    proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))\n",
    "    return proj_operator\n",
    "\n",
    "\n",
    "def generate_synthetic_data():\n",
    "    \"\"\" Synthetic binary data \"\"\"\n",
    "    rs = np.random.RandomState(0)\n",
    "    n_pts = 36.\n",
    "    x, y = np.ogrid[0:l, 0:l]\n",
    "    mask_outer = (x - l / 2) ** 2 + (y - l / 2) ** 2 < (l / 2) ** 2\n",
    "    mask = np.zeros((l, l))\n",
    "    points = l * rs.rand(2, n_pts)\n",
    "    mask[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1\n",
    "    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\n",
    "    res = np.logical_and(mask > mask.mean(), mask_outer)\n",
    "    return res - ndimage.binary_erosion(res)\n",
    "\n",
    "\n",
    "# Generate synthetic images, and projections\n",
    "l = 128\n",
    "proj_operator = build_projection_operator(l, l / 7.)\n",
    "data = generate_synthetic_data()\n",
    "proj = proj_operator * data.ravel()[:, np.newaxis]\n",
    "proj += 0.15 * np.random.randn(*proj.shape)\n",
    "\n",
    "# Reconstruction with L2 (Ridge) penalization\n",
    "rgr_ridge = Ridge(alpha=0.2)\n",
    "rgr_ridge.fit(proj_operator, proj.ravel())\n",
    "rec_l2 = rgr_ridge.coef_.reshape(l, l)\n",
    "\n",
    "# Reconstruction with L1 (Lasso) penalization\n",
    "# the best value of alpha was determined using cross validation\n",
    "# with LassoCV\n",
    "rgr_lasso = Lasso(alpha=0.001)\n",
    "rgr_lasso.fit(proj_operator, proj.ravel())\n",
    "rec_l1 = rgr_lasso.coef_.reshape(l, l)\n",
    "\n",
    "plt.figure(figsize=(8, 3.3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(data, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.title('original image')\n",
    "plt.subplot(132)\n",
    "plt.imshow(rec_l2, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.title('L2 penalization')\n",
    "plt.axis('off')\n",
    "plt.subplot(133)\n",
    "plt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.title('L1 penalization')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0,\n",
    "                    right=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Laplacian Tortoise and Gaussian Hare\n",
    "\n",
    "L1-norm can be also used in overdetermined systems:\n",
    "\n",
    "Instead of minimizing $\\Vert Ax - b \\Vert_2$, we minimize $\\Vert Ax - b \\Vert_1$.\n",
    "\n",
    "In statistics, it has been known for many years, that minimizing the sum of absolute values of residual is better with respect to \n",
    "\n",
    "outliers, than the least squares estimator.\n",
    "\n",
    "But solving **linear least squares** is typically so much faster!\n",
    "\n",
    "Now, however, methods for linear programming has advanced a lot.\n",
    "\n",
    "For further info, read the paper [Laplacian Tortoise and Gaussian Hare](https://projecteuclid.org/download/pdf_1/euclid.ss/1030037960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Are the signals really sparse?\n",
    "\n",
    "Let us take an image,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "lena = scipy.misc.lena()\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(lena)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Is it sparse?\n",
    "\n",
    "Of course, it is not sparse.\n",
    "\n",
    "But it is sparse in the **wavelet basis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea of discrete wavelet transform\n",
    "\n",
    "Wavelets are a very important concept. Before, you have heard about Fast Fourier Transform, which is an expansion of the function in the Fourier basis:\n",
    "\n",
    "$$f(x) \\approx \\sum_{k} c_k e^{ikx}, $$\n",
    "\n",
    "and sometimes the coefficients are sparse.\n",
    "\n",
    "However, trigonometric functions have **global support,** and thus a small change in only place changes coefficients everywhere.\n",
    "\n",
    "If there is a singularity at some point, all the coefficients start to decay slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 128\n",
    "x = np.linspace(-np.pi, np.pi, n)\n",
    "f1 = np.sqrt(np.abs(x))#np.sqrt(np.abs(np.sin(np.pi*x)))\n",
    "plt.semilogy(np.sort(abs(np.fft.fft(f1))))\n",
    "plt.title('Decay of Fourier coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to avoid global basis functions\n",
    "\n",
    "A useful idea (used, from example, in JPEG), is to use **window FFT**. You select a certain window, and do the transform only there.  \n",
    "\n",
    "But in this case you consider only one scale of the transformation.\n",
    "\n",
    "Wavelets bring this idea to the **multilevel** case, and give rise to the **multiresolution analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The simplest discrete wavelet transform\n",
    "\n",
    "The simplest discrete wavelet transform (DWT) is the **Haar wavelet transform.**\n",
    "\n",
    "It is defined as follows. \n",
    "\n",
    "Given a vector of length $2^d$, we compute\n",
    "\n",
    "$$x'_{2i} = \\frac{1}{2} (x_{2i+1} + x_{2i}), \\quad x'_{2i+1} = \\frac{1}{2}(x_{2i} + x_{2i+1}),$$\n",
    "\n",
    "i.e. we compute sums and differences of the neighbouring points. \n",
    "\n",
    "This is also a **window Fourier transform** with two points.\n",
    "\n",
    "The idea of wavelets is that we do not stop here: we take the sums, put them into the top $\\frac{n}{2}$ elements of the vector, \n",
    "\n",
    "and apply the Haar transform once more.\n",
    "\n",
    "Why it works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why it works\n",
    "\n",
    "If the vector is constant, after such transform we will get only one non-zero, i.e. compression.\n",
    "\n",
    "If it is **smooth**, than the differences will be much smaller, than the sums, so we can approximate the result by a sparse vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discrete wavelet transforms give wavelet bases\n",
    "\n",
    "DWT gives **wavelet bases**, in the standard form those bases are spanned by the **scaling and translation** of one mother function,\n",
    "\n",
    "$$\\psi_{j, k} = 2^{j/2} \\psi(2^j x - k),$$\n",
    "\n",
    "(for the FFT we only have translation).\n",
    "\n",
    "Design of the **mother function** $\\psi$ gives rise to many other wavelets, and probably the most well-known basis functions are \n",
    "\n",
    "wavelets from the **Daubechies** family (used in JPEG2000). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pywt #python wavelet package\n",
    "import matplotlib.pyplot as plt\n",
    "pywt.wavelist() #List of wavelets\n",
    "w = pywt.Wavelet('db2')\n",
    "phi, psi, x = w.wavefun(level=10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#ax.set_xlim(-.02,1.02)\n",
    "ax.plot(x, phi);\n",
    "ax.set_title('Daubechies wavelet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wavelet summary \n",
    "\n",
    "- The wavelet basis functions act locally, and the coefficients are **pseudo-sparse** (in the sense, many of them are small). \n",
    "- The more sparsity you want, the more local coefficients you need\n",
    "- Some properties (like orthogonality, smoothness) are difficult to get\n",
    "\n",
    "For more details, FastPDE course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparsity of Lena\n",
    "\n",
    "Let us test the sparsity of Lena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "lena = scipy.misc.lena()\n",
    "\n",
    "from pywt import multilevel\n",
    "coeffs = multilevel.dwt2(lena, 'haar')\n",
    "cA, (cH, cV, cD) = coeffs\n",
    "#plt.imshow(cD)\n",
    "n = lena.shape[0]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.imshow(np.bmat([[cA, cH], [cV, cD]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to compressed sensing\n",
    "\n",
    "Thus, given $Ax = y,$\n",
    "\n",
    "we first find the representation $x \\approx W s,$\n",
    "\n",
    "where $W$ is known, and try to solve for\n",
    "\n",
    "$$A W s \\approx y,$$\n",
    "\n",
    "with the assumption that $s$ is sparse.\n",
    "\n",
    "But how to find those **sparsest solutions?.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding sparse solutions of linear systems\n",
    "\n",
    "There are several methods for finding sparse solutions of overdetermined linear systems.\n",
    "\n",
    "For more details, you can read [the review](http://www.researchgate.net/profile/Alfred_Bruckstein/publication/220116567_From_Sparse_Solutions_of_Systems_of_Equations_to_Sparse_Modeling_of_Signals_and_Images/links/0912f5088e5bca1f3f000000.pdf).\n",
    "\n",
    "which we basically follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions and problem\n",
    "\n",
    "$$Ax = b.$$\n",
    "\n",
    "Q1: When can uniqueness of the sparsest solution be claimed?  \n",
    "Q2: Can a candidate solution be tested to verify its (global) optimality?  \n",
    "Q3: Can the solution be reliably and efficiently found in practice? and finally,  \n",
    "Q4: What performance guarantees can be given for various approximate and practical solvers?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uniqueness\n",
    "\n",
    "We need the new notion of the **rank** of the matrix.\n",
    "\n",
    "The rank of the matrix is the maximal number of linear independent columns of the matrix.\n",
    "\n",
    "The **spark** of a given matrix is the smallest number of linearly dependent columns of the matrix.\n",
    "\n",
    "From the definition, if we have\n",
    "\n",
    "$$Ax = 0,$$\n",
    "\n",
    "then $\\Vert x \\Vert_0 \\geq spark(A).$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theorem\n",
    "\n",
    "If the system of linear equations $Ax = b$ has a solution $x$ obeying $\\Vert x \\Vert_0 < spark(A)/2$, then this solution is the sparsest possible.\n",
    "\n",
    "**Proof:** If there is another solution with less zeros, then $\\Vert x \\Vert_0 + \\Vert_0 y \\Vert_0 \\geq \\Vert x - y \\Vert_0 \\geq spark(A)$.  \n",
    "\n",
    "Any alternative solution should have more than $spark(A)/2$ non-zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How large spark can be?\n",
    "\n",
    "The spark can not be larger than $n+1$. For a random matrix $A$, with probability $1$ we have $spark(A) = n + 1$. However, if $A$ has zero column, that it is a big problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uniqueness and mutual coherence\n",
    "\n",
    "Computing spark is as difficult, as solving the original problem, thus we need to have computable quantities. \n",
    "\n",
    "The **mutual coherence** is the simplest choice. \n",
    "\n",
    "It is defined as\n",
    "\n",
    "$$\\mu(A) = \\max_{1 \\leq k, j \\leq m} \\frac{a^*_k a_j}{\\Vert a_k \\Vert_2 \\Vert a_j \\Vert_2}.$$\n",
    "\n",
    "For a unitary matrix $\\mu$ is zero, for a general matrix it is possible and we want to make it as small as possible.\n",
    "\n",
    "For random orthogonal matrices $\\mu \\sim \\sqrt{\\frac{\\log(nm)}{n}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mutual coherence and spark\n",
    "\n",
    "Mutual coherence is easy to compute and provides a lower bound for spark:\n",
    "\n",
    "$$spark(A) \\geq 1 + \\frac{1}{\\mu(A)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Proof\n",
    "\n",
    "First, scale the columns of $A$ to make the $1$ in the Euclidean norm.\n",
    "\n",
    "The entries of the Gram matrix $G = A^* A$ satisfy \n",
    "\n",
    "$$\n",
    "    G_{kk} = 1, \\quad |G_{kj}| \\leq \\mu.\n",
    "$$\n",
    "\n",
    "Now consider an arbitrary $p \\times p$ minor of $G$. \n",
    "\n",
    "If this minor is diagonally dominant, then from the Gershgorin disk theorem then the submatrix of $G$ is positive definite and the columns are linearly independent. \n",
    "\n",
    "Condition $p < 1 + \\frac{1}{\\mu}$ guarantees the diagonal dominance of all $p \\times p$ minor of the Gram matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical algorithms\n",
    "\n",
    "Suppose spark(A) > 2 and the solution has only $1$ non-zero. \n",
    "\n",
    "Then we can do $m$ tests to recover the location.\n",
    "\n",
    "For $k_0$ non-zeros, we will have to test $\\mathcal{O}(m^{k_0})$ possible combinations.\n",
    "\n",
    "**We can do greedy method:**\n",
    "\n",
    "At each step, add just one index to the **active index set.**\n",
    "\n",
    "If we know the index set, we can compute the coefficients by solving linear least squares.\n",
    "\n",
    "The main question is which index to add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonal matching pursuit\n",
    "\n",
    "**Initialization:** Set $x_0 = 0$, compute initial residual $r_0 = b - Ax_0 = b.$ Set initial support to $S = \\{\\}.$.\n",
    "\n",
    "**Iteration:** Compute the errors $\\varepsilon(j) = \\min_{z_j} \\Vert a_j z_j - r_j \\Vert_2$ for all $j$ using the optimal choice.  \n",
    "               **Update support:** Find  a minimizer $j_0$ of $\\epsilon(j)$, and increase support.  \n",
    "               **Update provisional solution:** Find $x_k$ that minimizes $\\Vert A x - b \\Vert_2$ subject to support.  \n",
    "               **Update the residual:** $r_k = b - A x_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex relaxation techniques\n",
    "\n",
    "If we replace 0-norm by its closest convex \"friend\" $L1$-norm, we get the **Basis pursuit** problem:\n",
    "\n",
    "$$\\min_x \\Vert W x \\Vert_1, \\mbox{subject to } b = Ax,$$\n",
    "\n",
    "where $W$ is a **diagonal scaling matrix.**\n",
    "\n",
    "(since the 0-norm is insensitive to the norm of the columns, but $1$-norm is sensitive, a natural choice is $w_{ii} = \\Vert a_i \\Vert.$\n",
    "\n",
    "We can solve it using modern LP (linear programming) solvers (interior point, simplex, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Does greedy work?\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "For a system $Ax = b$ if the solution $x$ satisfies  \n",
    "\n",
    "$$\n",
    "    \\Vert x \\Vert_0 < \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right) \n",
    "$$\n",
    "\n",
    "The OGA run with threshold $\\epsilon_0 = 0$ is guaranteed to find it exactly.\n",
    "\n",
    "Let us try to do it on whiteboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Does Basis Pursuit work?\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "For a system $Ax = b$ if the solution $x$ satisfies  \n",
    "\n",
    "$$\n",
    "    \\Vert x \\Vert_0 < \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right) \n",
    "$$\n",
    "\n",
    "then there exists a unique solution of BP problem which is the unique solution of the sparse problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stability estimate\n",
    "\n",
    "If the right-hand side is known up to some error $\\delta$, then stability estimates can be also proven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Large-scale case\n",
    "\n",
    "How to solve the problems for the large-scale case? \n",
    "\n",
    "First, we formulate our problem using Lagrange multipliers:\n",
    "\n",
    "$$\\min_x \\lambda \\Vert x \\Vert_1 + \\frac{1}{2} \\Vert b - A x \\Vert^2_2$$\n",
    "\n",
    "What if $A$ is large and structured?\n",
    "\n",
    "There are many methods (iterative shrinkage, Bregman iteration, projection method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bregman iteration\n",
    "\n",
    "Let us discuss one approach. First we introduce a new variable $y = x$ \n",
    "\n",
    "$$\\min_{x, y, s} \\lambda \\Vert y \\Vert_1 + \\frac{1}{2} \\Vert A x - b \\Vert^2_2 + \\langle x - y, s \\rangle + \\mu \\Vert x - y \\Vert^2.$$\n",
    "\n",
    "Now we can do **alternating minimization:**\n",
    "\n",
    "The most important point, is that minimization over $y$ is reduced to the separate minimization of function\n",
    "\n",
    "$$\\min_{y_i} \\lambda |y|_i + (x_i - y_i) s_i + \\mu (x_i - y_i)^2,$$\n",
    "\n",
    "or equivalenly, minimization of\n",
    "\n",
    "\n",
    "$$\\min_{y_i} \\lambda |y|_i + \\mu(y_i - z_i)^2.$$\n",
    "\n",
    "The solution is given by the **soft thresholding function:**\n",
    "\n",
    "$$y_i = shrink\\left(z_i, \\frac{\\lambda}{\\mu}\\right),$$\n",
    "\n",
    "and soft thresholding is defined as\n",
    "\n",
    "$$\n",
    "   shrink(x, \\delta)  = \\begin{cases} 0, \\quad &|x| < \\delta  \\\\\n",
    "   x - \\delta, \\quad &|x| \\geq \\delta\n",
    "   \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bregman iteration (cont)\n",
    "\n",
    "$$\\min_{x, y, s} \\lambda \\Vert y \\Vert_1 + \\frac{1}{2} \\Vert A x - b \\Vert^2_2 + \\langle x - y, s \\rangle + \\mu \\Vert x - y \\Vert^2.$$\n",
    "\n",
    "Summary:\n",
    "\n",
    "1. Least squares update for $x$\n",
    "2. Shrinkage update for $y$\n",
    "3. Gradient update for $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quest for optimal basis\n",
    "\n",
    "Ok, if we know that if the solution is sparse, we are able to recover it.\n",
    "\n",
    "We also know, that wavelet basis gives a sparse representation. \n",
    "\n",
    "But this is a **particular basis** that is not adapted to the particular application.\n",
    "\n",
    "The next step is to adapt the basis itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem setting\n",
    "\n",
    "Suppose we have a **database** of vectors $a_1, \\ldots, a_N$. This can be, for example, **blocks** of images from the database.\n",
    "\n",
    "Then we want to find a **dictionary** $d_1, \\ldots, d_M$, $M \\ll N$ such that every $a_k$ can be represented as a **sparse linear combination** of the dictionary vectors:\n",
    "\n",
    "$$a_k = D c_k, $$\n",
    "\n",
    "and $c_k$ is sparse.\n",
    "\n",
    "The idea (called **K-SVD**) is to formulate the loss using $L_1$ - norm and then iteratively update the dictionary and coefficients. \n",
    "\n",
    "If you update only one atom from the dictionary, it is possible to write the optimization problem in terms of rank-$1$ approximation of the auxiliary matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Many more topics & applications not covered \n",
    "- $L_1$ norm penalizes sparsity, so it is an excellent prior, if the right basis is found (K-SVD can be used)\n",
    "- OMP & BP methods are the methods of choice for small-scale, \n",
    "- Bregman iteration (and its siblings like linearized Bregman) are the most efficient for Large-Scale Problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Next lecture\n",
    "\n",
    "On the next lecture we will talk about **multilinear algebra** and **tensors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
